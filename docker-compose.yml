services:
  llm:
    image: ollama/ollama:latest
    profiles: ["linux"]
    volumes:
      - ./data/ollama:/root/.ollama
    ports:
      - 11434:11434
    networks:
      - corgi-net
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1 
              capabilities: [ gpu ]

  neo:
    image: neo4j:5.13
    ports:
      - 7687:7687
      - 7474:7474
    volumes:
      - ./data/neo:/data
    environment:
      - NEO4J_AUTH=${NEO4J_USERNAME-neo4j}/${NEO4J_PASSWORD-password}
      - NEO4J_PLUGINS=["apoc"]
      - NEO4J_db_tx__log_rotation_retention__policy=false
    healthcheck:
        test: ["CMD-SHELL", "wget --no-verbose --tries=1 --spider localhost:7474 || exit 1"]
        interval: 5s
        timeout: 3s
        retries: 5
    networks:
      - corgi-net

  py-loader:
    image: corgiai/py-loader:latest
    build:
      context: ./apps/py-loader
      additional_contexts:
        config: ./config
    networks:
      - corgi-net
    ports:
      - 8081:8080
      - 8100:8100
    volumes:
      - ./data/embedding_model:/embedding_model
    depends_on:
      pull-model:
        condition: service_completed_successfully
      neo:
        condition: service_healthy
    x-develop: # x here is important for some reason
      watch:
        - action: rebuild
          path: ./apps/py-loader

  py-chat:
    image: corgiai/py-chat:latest
    build:
      context: ./apps/py-chat
      additional_contexts:
        config: ./config
    networks:
      - corgi-net
    ports:
      - 8200:8200
    volumes:
      - ./data/embedding_model:/embedding_model
    depends_on:
      pull-model:
        condition: service_completed_successfully
      neo:
        condition: service_healthy
    x-develop: # x here is important for some reason
      watch:
        - action: rebuild
          path: ./apps/py-chat
  
  pull-model:
    image: corgiai/pull-model:latest
    build:
      dockerfile: pull_model.Dockerfile
    environment:
      - OLLAMA_BASE_URL=${OLLAMA_BASE_URL-http://host.docker.internal:11434}
      - LLM=${LLM-llama2}
    networks:
      - corgi-net
  
  spa:
    image: corgiai/spa:latest
    build:
      context: ./apps/corgiai-spa
      args:
        - MODE=tailscale
    networks:
      - corgi-net
    ports:
      - 8080:80
    x-develop: # x here is important for some reason
      watch:
        - action: rebuild
          path: ./apps/corgiai-spa

networks:
  corgi-net: