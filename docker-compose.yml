version: "3"

services:
  llm:
    image: ollama/ollama:latest
    profiles: ["linux"]
    volumes:
      - ./data/ollama:/root/.ollama
    ports:
      - 11434:11434
    networks:
      - corgi-net
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1 
              capabilities: [ gpu ]

  neo:
    image: neo4j:5.13
    ports:
      - 7687:7687
      - 7474:7474
    volumes:
      - ./data/neo:/data
    environment:
      - NEO4J_AUTH=${NEO4J_USERNAME-neo4j}/${NEO4J_PASSWORD-password}
      - NEO4J_PLUGINS=["apoc"]
      - NEO4J_db_tx__log_rotation_retention__policy=false
    healthcheck:
        test: ["CMD-SHELL", "wget --no-verbose --tries=1 --spider localhost:7474 || exit 1"]
        interval: 5s
        timeout: 3s
        retries: 5
    networks:
      - corgi-net

  py-loader:
    image: ghcr.io/blazekill/corgiai-py-loader:latest
    build:
      context: ./apps/py-loader
      additional_contexts:
        config: ./config
    networks:
      - corgi-net
    ports:
      - 8081:8080
      - 8100:8100
    volumes:
      - ./data/embedding_model:/embedding_model
    depends_on:
      pull-model:
        condition: service_completed_successfully
      neo:
        condition: service_healthy

  py-chat:
    image: ghcr.io/blazekill/corgiai-py-chat:latest
    build:
      context: ./apps/py-chat
      additional_contexts:
        config: ./config
    networks:
      - corgi-net
    ports:
      - 8200:8200
    volumes:
      - ./data/embedding_model:/embedding_model
    depends_on:
      pull-model:
        condition: service_completed_successfully
      neo:
        condition: service_healthy

  quiz-api:
    image: ghcr.io/blazekill/corgiai-quiz-api:latest
    build:
      context: ./apps/CorgiAPI
      dockerfile: ./QuizAPI/Dockerfile
      additional_contexts:
        config: ./config
    networks:
      - corgi-net
    ports:
      - 8300:8300
    depends_on:
      pull-model:
        condition: service_completed_successfully
      neo:
        condition: service_healthy
  
  pull-model:
    image: ghcr.io/blazekill/corgiai-pull-model:latest
    build:
      context: ./apps/pull-model
    environment:
      - OLLAMA_BASE_URL=${OLLAMA_BASE_URL-http://host.docker.internal:11434}
      - LLM=${LLM-llama2}
    networks:
      - corgi-net

  app:
    image: ghcr.io/blazekill/corgiai-app:latest
    build:
      dockerfile: local.Dockerfile
      context: ./apps/corgiai
    networks:
      - corgi-net
    ports:
      - 8080:8080
    depends_on:
      pull-model:
        condition: service_completed_successfully
      neo:
        condition: service_healthy

networks:
  corgi-net: